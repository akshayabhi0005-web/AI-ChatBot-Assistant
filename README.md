# AI-ChatBot-Assistant
ğŸ¤– Intelligent Voice-Controlled AI Assistant with Dual Display Feedback
An advanced always-listening AI assistant built on Raspberry Pi, designed to deliver hands-free voice interaction, real-time visual feedback, and lifelike animated expressions.
This project blends AI, speech processing, embedded systems, and human-computer interaction into a single continuously running system.

ğŸ™ï¸ How It Works

The assistant operates fully autonomously, without manual input:

ğŸ§ Listens continuously to user speech through a microphone

ğŸ“ Converts speech to text using real-time speech recognition

ğŸ§  Processes queries via a large language model using the Groq API

ğŸ—£ï¸ Responds using offline text-to-speech synthesis

ğŸ” Automatically resets after every interaction for uninterrupted operation

No freezing, no manual restarts â€” just seamless conversation.

ğŸ–¥ï¸ Dual-Display Visual Feedback
ğŸ“º ST7735 TFT Display (SPI)

Used for textual interaction and system status:

ğŸ’¬ Displays user questions and AI responses

ğŸ“œ Automatically wraps and scrolls long messages

âš™ï¸ Shows system states like:

Listening

Recognizing

Thinking

Speaking

Optimized to fit limited screen resolution while remaining readable and responsive.

ğŸ‘€ SSD1306 OLED Display (I2C)

Provides expressive animated eyes, giving the assistant personality:

ğŸ˜ Idle / Sleeping

ğŸ‘‚ Listening

ğŸ§  Thinking

ğŸ˜Š Happy responses

ğŸ˜´ Sleep & blink animations

Animations dynamically change based on assistant state, creating a natural and intuitive human-machine interaction.

ğŸ§© Software Architecture

The system follows a modular and fault-tolerant design:

ğŸ›ï¸ Controller Layer

Manages system flow and state transitions

Starts and stops animations dynamically

Prevents I2C conflicts and display crashes

ğŸ§  AI Processing Module

Handles Groq API communication

Formats prompts and responses

ğŸ¤ Speech Recognition Module

Continuous microphone input

Robust noise calibration

ğŸ”Š Text-to-Speech Module

Offline speech synthesis using espeak

Synchronized with animations

ğŸ–¼ï¸ Display Modules

TFT text rendering and scrolling

OLED eye animations

External animation scripts are launched and terminated dynamically, ensuring smooth performance and stable hardware communication.

ğŸ“¦ Libraries & Technologies Used
ğŸ§  AI & Networking

requests â€“ API communication

Groq API â€“ Large Language Model processing

ğŸ™ï¸ Voice Interface

speech_recognition â€“ Speech-to-text

espeak â€“ Offline text-to-speech

ğŸ–¥ï¸ Display & Graphics

st7735 â€“ TFT display control

luma.oled â€“ OLED display and animations

Pillow (PIL) â€“ Text rendering and graphics

âš™ï¸ System & Control

threading, subprocess â€“ Process and animation control

dotenv â€“ Secure API key management

ğŸ”Œ Hardware Interfaces

SPI â€“ TFT display

I2C â€“ OLED display

Carefully optimized to avoid bus collisions and runtime crashes.

ğŸš€ Key Features

ğŸ” Continuous, non-blocking operation

ğŸ‘€ Animated eyes for lifelike interaction

ğŸ“œ Auto-scrolling text on TFT

âš¡ Fast AI responses via Groq

ğŸ§  Modular, extensible codebase

ğŸ› ï¸ Designed for resource-constrained systems

ğŸ§ª Learning & Use Cases

This project is ideal for:

ğŸ§‘â€ğŸ’» Embedded AI experimentation

ğŸ¤ Human-Computer Interaction (HCI) research

ğŸ§  Voice-based interfaces

ğŸ« Engineering mini / major projects

ğŸ¤– Smart assistants & robotics prototypes

ğŸ”® Future Enhancements

ğŸ—£ï¸ Wake-word detection

ğŸŒ Offline language models

ğŸ­ More facial expressions

ğŸšï¸ Custom personalities

ğŸ“¡ IoT and sensor integration

â­ Final Note

This project demonstrates practical AI deployment on embedded hardware, combining speech, vision, animation, and intelligence into a compact Raspberry Pi system.
